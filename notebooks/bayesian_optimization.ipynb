{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.acquisition import ExpectedImprovement, qExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "from uncertaintylearning.utils import (FixedKernelDensityEstimator, CVKernelDensityEstimator,\n",
    "                                       create_network, create_optimizer, create_multiplicative_scheduler)\n",
    "from uncertaintylearning.models import EpistemicPredictor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from botorch.test_functions.synthetic import Ackley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0\n",
    "\n",
    "ackley_dim = 10\n",
    "\n",
    "def f(X, noise=noise):\n",
    "    return - Ackley(ackley_dim, noise)(X).unsqueeze(1)\n",
    "    # return torch.sin(X) * torch.cos(5 * X) * torch.cos(22 * X) + noise * torch.randn_like(X)\n",
    "\n",
    "\n",
    "bounds = (-1, 2)\n",
    "bounds = (-5, 10)\n",
    "# f, bounds = repeat(f, bounds)\n",
    "# f, bounds = repeat(f, bounds)\n",
    "\n",
    "X_init = (bounds[1] - bounds[0]) * torch.rand(20, ackley_dim) + bounds[0]\n",
    "Y_init = f(X_init)\n",
    "\n",
    "if ackley_dim == 1:\n",
    "    X = torch.arange(bounds[0], bounds[1], 0.01).reshape(-1, 1)\n",
    "    Y = f(X, 0)\n",
    "\n",
    "\n",
    "    # Plot optimization objective with noise level \n",
    "    plt.plot(X, Y, 'y--', lw=2, label='Noise-free objective')\n",
    "    plt.plot(X, f(X), 'bx', lw=1, alpha=0.1, label='Noisy samples')\n",
    "    plt.plot(X_init, Y_init, 'kx', mew=3, label='Initial samples')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(model_name, X_init, Y_init, plot=False, n_steps=5, epochs=15, iid_ratio=5, **kwargs):\n",
    "\n",
    "    full_train_X = X_init\n",
    "    full_train_Y = Y_init\n",
    "    \n",
    "    f_losses = []\n",
    "    e_losses = []\n",
    "    a_losses = []\n",
    "    state_dict = None\n",
    "    max_value_per_step = [full_train_Y.max().item()]\n",
    "\n",
    "    full_train_Y_2 = None\n",
    "    # full_train_Y_2 = f(full_train_X)\n",
    "    for step in range(n_steps):\n",
    "        if model_name == EpistemicPredictor:\n",
    "            density_estimator = FixedKernelDensityEstimator('exponential', 0.1)\n",
    "            if len(full_train_X) > 50:\n",
    "                iid_ratio = 1\n",
    "            model = model_name(full_train_X, full_train_Y, train_Y_2=full_train_Y_2, density_estimator=density_estimator,\n",
    "                               iid_ratio=iid_ratio, **kwargs)\n",
    "            if state_dict is not None:\n",
    "                model.load_state_dict(state_dict)\n",
    "            for _ in range(epochs):\n",
    "                losses = model.fit()\n",
    "                f_losses.append(np.mean(losses['f']))\n",
    "                a_losses.append(np.mean(losses['a']))\n",
    "                e_losses.append(np.mean(losses['e']))\n",
    "            \n",
    "        elif model_name == SingleTaskGP:\n",
    "            model = model_name(full_train_X, full_train_Y)\n",
    "            mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "            if state_dict is not None:\n",
    "                model.load_state_dict(state_dict)\n",
    "            fit_gpytorch_model(mll)\n",
    "        else:\n",
    "            raise Exception('Not sure this would work !')\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(step, ':', full_train_Y.max().item())\n",
    "        EI = ExpectedImprovement(model, full_train_Y.max().item())\n",
    "\n",
    "        \n",
    "\n",
    "        bounds_t = torch.FloatTensor([[bounds[0]] * ackley_dim, [bounds[1]] * ackley_dim])\n",
    "        candidate, acq_value = optimize_acqf(\n",
    "            EI, bounds=bounds_t, q=1, num_restarts=15, raw_samples=64,\n",
    "        )\n",
    "\n",
    "        if ackley_dim == 1 and plot:\n",
    "            eis = EI(X.unsqueeze(1)).detach()\n",
    "            max_ei, argmax_ei = torch.max(eis, 0)\n",
    "            xmax = X[argmax_ei].item()\n",
    "            max_ei = max_ei.item()\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 7))\n",
    "            # Plot optimization objective with noise level\n",
    "            ax1.plot(X, Y, 'y--', lw=2, label='Noise-free objective')\n",
    "            # ax1.plot(X, f(X), 'rx', lw=1, alpha=0.2, label='Noisy samples')\n",
    "            ax1.plot(full_train_X, full_train_Y, 'kx', mew=3, label='Used for training')\n",
    "\n",
    "            ax1.plot(X, model(X).mean.detach().squeeze(), label='mean pred')\n",
    "            ax1.fill_between(X.numpy().ravel(),\n",
    "                             model(X).mean.detach().numpy().ravel() - model(X).stddev.detach().numpy().ravel(),\n",
    "                             model(X).mean.detach().numpy().ravel() + model(X).stddev.detach().numpy().ravel(),\n",
    "                             alpha=.4)\n",
    "\n",
    "            if isinstance(model, EpistemicPredictor):\n",
    "                ax1.plot(X, model.density_estimator.score_samples(X), 'r-', label='density')\n",
    "                \n",
    "            # ax1.set_ylim(-3, 2)\n",
    "\n",
    "            ax2.plot(X, eis, 'r-', label='EI')\n",
    "            ax2.plot(X, [max_ei] * len(X), 'b--')\n",
    "            ax2.plot([xmax] * 100, torch.linspace(0, max_ei, 100), 'b--')\n",
    "            ax1.legend()\n",
    "            ax2.legend()\n",
    "            \n",
    "            ax3.plot(f_losses, label='f')\n",
    "            ax3.plot(e_losses, label='e')\n",
    "            ax3.plot(a_losses, label='a')\n",
    "            # ax3.set_ylim(0, 10)\n",
    "            ax3.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        full_train_X = torch.cat([full_train_X, candidate])\n",
    "        full_train_Y = torch.cat([full_train_Y, f(candidate)])\n",
    "        if model_name == EpistemicPredictor and full_train_Y_2 is not None:\n",
    "            full_train_Y_2 = torch.cat([full_train_Y_2, f(candidate)])\n",
    "\n",
    "        max_value_per_step.append(full_train_Y.max().item())\n",
    "\n",
    "    return max_value_per_step, f_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING THESE 3 SEEDS ENSURES DETERMINISTIC RESULTS\n",
    "# GP IS DETERMINISTIC given X_init and Y_init\n",
    "\n",
    "# density_estimator = FixedKernelDensityEstimator('exponential', 0.05)\n",
    "#torch.manual_seed(8)\n",
    "networks = {'a_predictor': create_network(1, 1, 64, 'tanh', True),\n",
    "            'e_predictor': create_network(2, 1, 64, 'relu', True),\n",
    "            'f_predictor': create_network(1, 1, 64, 'relu', False)\n",
    "            }\n",
    "\n",
    "optimizers = {'a_optimizer': create_optimizer(networks['a_predictor'], 1e-3),\n",
    "              'e_optimizer': create_optimizer(networks['e_predictor'], 1e-3),\n",
    "              'f_optimizer': create_optimizer(networks['f_predictor'], 1e-3)\n",
    "              }\n",
    "\n",
    "X_init = (bounds[1] - bounds[0]) * torch.rand(20, 1) + bounds[0]\n",
    "Y_init = f(X_init)\n",
    "\n",
    "_, = optimize(EpistemicPredictor, X_init, Y_init, plot=True, n_steps=15, networks=networks, optimizers=optimizers,\n",
    "    split_seed=1, dataloader_seed=2, iid_ratio=5, epochs=50, retrain=False)\n",
    "# optimize(SingleTaskGP, X_init, Y_init, plot=True, n_steps=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/2\n",
      "0 : -11.211603164672852\n",
      "10 : -1.9099626541137695\n",
      "20 : -1.9099626541137695\n",
      "30 : -1.9099626541137695\n",
      "40 : -1.9099626541137695\n"
     ]
    }
   ],
   "source": [
    "# Simple comparison of EP and GP !!\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "n_runs = 2\n",
    "n_steps = 500\n",
    "gp_runs = np.zeros((n_runs, n_steps + 1))\n",
    "ep_runs = np.zeros((n_runs, n_steps + 1))\n",
    "\n",
    "\n",
    "for i in range(0, n_runs):\n",
    "    torch.manual_seed(10 * i+1)\n",
    "    X_init = (bounds[1] - bounds[0]) * torch.rand(20, ackley_dim) + bounds[0]\n",
    "    Y_init = f(X_init)\n",
    "    \n",
    "    networks = {'a_predictor': create_network(ackley_dim, 1, 64, 'tanh', True),\n",
    "                'e_predictor': create_network(1 + ackley_dim, 1, 64, 'relu', True),\n",
    "                'f_predictor': create_network(ackley_dim, 1, 64, 'relu', False)\n",
    "                }\n",
    "\n",
    "    optimizers = {'a_optimizer': create_optimizer(networks['a_predictor'], 1e-3),\n",
    "                  'e_optimizer': create_optimizer(networks['e_predictor'], 1e-4),\n",
    "                  'f_optimizer': create_optimizer(networks['f_predictor'], 1e-3)\n",
    "                 }\n",
    "\n",
    "    print(f\"Run {i+1}/{n_runs}\")\n",
    "    # gp_runs[i], _ = optimize(SingleTaskGP, X_init, Y_init, plot=False, n_steps=n_steps)\n",
    "\n",
    "    ep_runs[i], _ = optimize(EpistemicPredictor, X_init, Y_init, plot=False, n_steps=n_steps, networks=networks,\n",
    "                             optimizers=optimizers, split_seed=1, dataloader_seed=2, batch_size=64, iid_ratio=5, epochs=50, retrain=False)\n",
    "\n",
    "    # plt.errorbar(range(1 + n_steps), gp_runs[:i+1].mean(0),  .2*gp_runs[:i+1].std(0), label='Average maximum value reached by GP')\n",
    "    plt.errorbar(range(1 + n_steps), ep_runs[:i+1].mean(0),  .2*ep_runs[:i+1].std(0), label='Average maximum value reached by EP')\n",
    "\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('ERROR BARS ARE ONLY 0.2 * STD !! ')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
